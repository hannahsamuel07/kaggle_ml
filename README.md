###### Intro to ML Kaggle Course
- Decision Tree  
- Model validation with Mean Absolute Error  
- Improving overfitting/underfitting with max leaf nodes  
- Upgrade to Random Forest (average of decision trees)  

###### Intermediate ML
- Dealing with missing values (drop, imputation, imputation with extension)  
- Handling categorical vars (drop, ordinal encoding, one-hot encoding)  
- Pipelines to bundle data preprocessing and modeling code
- Cross-validation to break the set into folds for improving model accuracy
- XGBoost parameter tuning and extreme gradient boosting (performance and speed focused)
- fixing data leakage(train test contamination and target leakage) by cross validating and dropping leaky variables

###### Intro To Deep Learning
- building neural networks with neurons and creating stacks of layers that accept input features using Keras
- using activation functions and stacking dense layers to make a full network
- training data in mini-batches (stochastic gradient descent), trying learning rates and batch sizes
- using callback early stopping, mini delta, patience, and restore weight parameters to prevent the model from learning too little from the signal and learning too much from noise
- dropout and batch normalization
- binary classification
